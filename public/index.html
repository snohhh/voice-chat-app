<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Voice Chat Assistant</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
        }
        #conversation {
            height: 400px;
            border: 1px solid #ccc;
            margin: 20px 0;
            padding: 20px;
            overflow-y: auto;
        }
        .controls {
            display: flex;
            gap: 10px;
            margin: 20px 0;
        }
        .message {
            margin: 10px 0;
            padding: 10px;
            border-radius: 5px;
        }
        .user-message {
            background-color: #e3f2fd;
        }
        .assistant-message {
            background-color: #f5f5f5;
        }
        #status {
            color: #666;
            margin-top: 10px;
        }
        .error {
            color: #f44336;
            padding: 10px;
            margin: 10px 0;
            border-radius: 4px;
            background-color: #ffebee;
        }
    </style>
</head>
<body>
    <h1>Voice Chat Assistant</h1>
    <div id="conversation"></div>
    <div id="status"></div>
    <div class="controls">
        <button id="startRecording">Start Recording</button>
        <button id="stopRecording" disabled>Stop Recording</button>
        <button id="downloadTranscript">Download Conversation</button>
    </div>

    <script>
        let mediaRecorder = null;
        let audioChunks = [];
        let conversationHistory = [];
        let stream = null;
        
        const API_BASE_URL = '/api/openai';
        const startRecordingButton = document.getElementById('startRecording');
        const stopRecordingButton = document.getElementById('stopRecording');
        const downloadButton = document.getElementById('downloadTranscript');
        const conversationDiv = document.getElementById('conversation');
        const statusDiv = document.getElementById('status');

        function updateStatus(message) {
            statusDiv.textContent = message;
        }

        function showError(message) {
            const errorDiv = document.createElement('div');
            errorDiv.className = 'error';
            errorDiv.textContent = message;
            conversationDiv.appendChild(errorDiv);
            conversationDiv.scrollTop = conversationDiv.scrollHeight;
        }

        async function startRecording() {
            try {
                // Reset audio chunks
                audioChunks = [];
                
                // Get microphone permission
                stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                
                // Create new MediaRecorder instance
                mediaRecorder = new MediaRecorder(stream);

                // Set up event handlers
                mediaRecorder.ondataavailable = (event) => {
                    if (event.data.size > 0) {
                        audioChunks.push(event.data);
                    }
                };

                mediaRecorder.onstop = async () => {
                    try {
                        updateStatus('Processing audio...');
                        if (audioChunks.length === 0) {
                            throw new Error('No audio data recorded');
                        }
                        const audioBlob = new Blob(audioChunks, { type: 'audio/wav' });
                        await processAudioAndGetResponse(audioBlob);
                    } catch (error) {
                        console.error('Error in onstop handler:', error);
                        showError(`Error processing recording: ${error.message}`);
                    } finally {
                        updateStatus('');
                    }
                };

                // Start recording
                mediaRecorder.start();
                startRecordingButton.disabled = true;
                stopRecordingButton.disabled = false;
                updateStatus('Recording...');
            } catch (error) {
                console.error('Error starting recording:', error);
                showError(`Error starting recording: ${error.message}`);
                updateStatus('');
            }
        }

        function stopRecording() {
            if (mediaRecorder && mediaRecorder.state !== 'inactive') {
                mediaRecorder.stop();
                if (stream) {
                    stream.getTracks().forEach(track => track.stop());
                }
                startRecordingButton.disabled = false;
                stopRecordingButton.disabled = true;
                updateStatus('Processing...');
            }
        }

        async function processAudioAndGetResponse(audioBlob) {
            try {
                // First, transcribe the audio
                updateStatus('Transcribing audio...');
                const formData = new FormData();
                formData.append('file', audioBlob, 'audio.wav');
                formData.append('model', 'whisper-1');

                const transcriptionResponse = await fetch(`${API_BASE_URL}/v1/audio/transcriptions`, {
                    method: 'POST',
                    body: formData
                });

                if (!transcriptionResponse.ok) {
                    const errorData = await transcriptionResponse.json();
                    throw new Error(`Transcription failed: ${errorData.error?.message || 'Unknown error'}`);
                }

                const transcriptionData = await transcriptionResponse.json();
                if (!transcriptionData.text) {
                    throw new Error('No speech detected');
                }

                const userMessage = transcriptionData.text;
                addMessageToConversation('user', userMessage);

                // Get ChatGPT response
                updateStatus('Getting response...');
                const chatResponse = await fetch(`${API_BASE_URL}/v1/chat/completions`, {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json'
                    },
                    body: JSON.stringify({
                        model: 'gpt-4',
                        messages: [{ role: 'user', content: userMessage }]
                    })
                });

                if (!chatResponse.ok) {
                    const errorData = await chatResponse.json();
                    throw new Error(`Chat response failed: ${errorData.error?.message || 'Unknown error'}`);
                }

                const chatData = await chatResponse.json();
                const assistantMessage = chatData.choices[0].message.content;
                addMessageToConversation('assistant', assistantMessage);

                // Convert to speech
                updateStatus('Converting to speech...');
                const speechResponse = await fetch(`${API_BASE_URL}/v1/audio/speech`, {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json'
                    },
                    body: JSON.stringify({
                        model: 'tts-1',
                        input: assistantMessage,
                        voice: 'alloy'
                    })
                });

                if (!speechResponse.ok) {
                    const errorData = await speechResponse.json();
                    throw new Error(`Text-to-speech failed: ${errorData.error?.message || 'Unknown error'}`);
                }

                const responseAudioBlob = await speechResponse.blob();
                const audioUrl = URL.createObjectURL(responseAudioBlob);
                const audio = new Audio(audioUrl);
                await audio.play();

            } catch (error) {
                console.error('Error processing audio:', error);
                showError(`Error: ${error.message}`);
            } finally {
                updateStatus('');
            }
        }

        function addMessageToConversation(role, content) {
            const messageDiv = document.createElement('div');
            messageDiv.className = `message ${role}-message`;
            messageDiv.textContent = content;
            conversationDiv.appendChild(messageDiv);
            conversationDiv.scrollTop = conversationDiv.scrollHeight;

            conversationHistory.push({
                timestamp: new Date().toISOString(),
                role: role,
                content: content
            });
        }

        function downloadTranscript() {
            const transcript = conversationHistory.map(msg => 
                `[${msg.timestamp}] ${msg.role}: ${msg.content}`
            ).join('\n\n');

            const blob = new Blob([transcript], { type: 'text/plain' });
            const url = URL.createObjectURL(blob);
            const a = document.createElement('a');
            a.href = url;
            a.download = `conversation-${new Date().toISOString()}.txt`;
            document.body.appendChild(a);
            a.click();
            document.body.removeChild(a);
            URL.revokeObjectURL(url);
        }

        // Add event listeners
        startRecordingButton.addEventListener('click', startRecording);
        stopRecordingButton.addEventListener('click', stopRecording);
        downloadButton.addEventListener('click', downloadTranscript);
    </script>
</body>
</html>